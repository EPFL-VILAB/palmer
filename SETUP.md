# How to Navigate this Codebase?

## Dependencies
<ul>
<li> Install <a href="https://github.com/Farama-Foundation/ViZDoom">VizDOOM</a>.</li>
<li> (Optional) If you want to run experiments in Habitat install <a href="https://github.com/facebookresearch/habitat-sim">habitat-sim</a> and <a href="https://github.com/facebookresearch/habitat-lab">habitat-lab</a>.</il>
<li> (Optional) If you want to run experiments in Maze2D, install the <a href="https://github.com/Farama-Foundation/D4RL">D4RL benchmark</a>.</li>
<li> Finally, install all other packages listed in <a href="https://github.com/EPFL-VILAB/palmer/blob/main/requirements.txt">requirements.txt</a>.</li>
</ul>

## Further Explanation of the Source-Code
### /agents
Contains definitions for the network architectures and subroutines employed in our planning algorithms. <em><b>TODO: Modify configurations and set paths before running the code.</b></em>
<ul>
  <li><b>backbone_models.py:</b> Common building blocks for implementing configurable ResNet backbones. Mostly borrowed from <a href="https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/resnet.py">here</a>.</li>
  <li><b>mpc_agent.py:</b> An agent that combines a learned latent transition model with a random-shooting optimizer to build an MPC loop for planning. Used as a baseline in our paper.</li>
  <li><b>mpc_agent_graph_PRM.py:</b> Implementation of an agent that plans using our proposed R-PRM algorithm. </li>
  <li><b>mpc_agent_graph_PRM_retrieval.py:</b> Extends mpc_agent_graph_PRM.py with additional functionality to plan with R-PRM at every timestep to build an MPC loop. This is the main method we used in the paper for our evaluations.</li>
  <li><b>mpc_agent_graph_RRT.py:</b> Implementation of an agent that plans using our proposed R-RRT algorithm. </li>
  <li><b>mpc_agent_graph_RRT_goal_directed.py:</b> A variant of R-RRT that biases the trajectory retrievals towards the goal-state by using the learned latent distance metric. Doesn't work better then the original R-RRT algorithm. </li>
  <li><b>mpc_agent_graph_RRT_star.py:</b> Implementation of an agent that plans using our proposed R-RRT* algorithm. </li>
  <li><b>mpc_agent_graph_RRT_star_goal_directed.py:</b> A variant of R-RRT* that biases the trajectory retrievals towards the goal-state by using the learned latent distance metric. Doesn't work better then the original R-RRT* algorithm. </li>
  <li><b>mpc_model.py:</b> Implements the network architecture and training functionality commonly shared by R-PRM, R-RRT, R-RRT*.</li>
  <li><b>random_agent.py:</b> Samples uniformly random actions. Mainly employed to collect the initial offline replay-buffer. </li>
  <li><b>test_*.ipynb:</b> Notebooks that contain tests and visualizations for the agents listed above. 
  <li><b>train_mpc_model.py:</b> A simple trainer for the networks implemented in mpc_model.py</li>
</ul>
  
### /dataloaders
Contains a single file datasets.py that implements all pytorch datasets used in experiments.

### /envs
Contains implementations for the VizDOOM environments used in experiments.
<ul>
  <li><b>vizdoom_env.py:</b> A simple abstract gym wrapper class.</li>
  <li><b>run_env_interactive.ipynb:</b> Tests and visualizations for the implemented environments.</li>
</ul>

### /envs/game_registry
Contains implementations for the VizDOOM environments used in experiments.
<ul>
  <li><b>game_template.py:</b> Another gym wrapper class with more fine-grained control over the environment.</li>
  <li><b>simple_exploration_game.cfg:</b> A configuration file that exposes <a href="https://doomwiki.org/wiki/ZDoom">ZDoom</a> variables.</li>
  <li><b>simple_exploration_game.py:</b> Implements the functionality for resetting, updating, and visualizing game states in VizDoom.</li>
  <li><b>*.wad:</b> Map definitions in native ZDoom format.</li>
  <li><b>simple_exploration_game_stochastic.py:</b> Same as simple_exploration_game.py, but adds noise when updating the game state to implement stochastic environment dynamics.</li>
</ul>

## /eval
Contains scripts for generating training and evaluation data, as well as for logging evaluation metrics. em><b>TODO: Modify configurations and set paths before running the code.</b></em>
<ul>
  <li><b>eval_retrieval_prm_agent.py:</b> Uses the training data generated by generate_eval_data_prm_agent.py to evaluate the success ratio for the R-PRM policy implemented in mpc_agent_graph_PRM_retrieval.py. This is the script that produced Fig.4 in our paper.</li>
  <li><b>eval_retrieval_prm_agent_stochastic.py:</b> Same as eval_retrieval_prm_agent.py, but uses simple_exploration_game_stochastic.py to evaluate on a stochastic environment instead.</li>
  <li><b>generate_eval_data_distance.py:</b> Generates the data for evaluating the learned latent distance metric. This is the script that produced Fig.3 in our paper.</li>
  <li><b>generate_eval_data_prm_agent.py:</b>  Generates the data for evaluating a navigation agent. Uses rejection sampling to create start/goal state pairs whose geodesic distance lies within a given interval.</li>
  <li><b>generate_random_walk_data.py:</b> Creates the initial offline replay-buffer.</li>
  <li><b>random_walk_data_project_latent.py:</b> Given the perceptual backbone of a navigation agent, extracts and stores the latent representations of all images created by generate_random_walk_data.py. Used for nearest-neighbor retrievals.</li>
</ul>

